<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Converser (System Card Added)</title>
        <style>
            body {font-family: sans-serif; color: #111111; padding: 2%; background-color: #eeeeee; max-width: 800px; margin: 0 auto;}
            h2 {color: #111111}
            .container {background: #ffffff; padding: 20px; border-radius: 8px; box-shadow: 0 0 10px grey}
            button {font-family: sans-serif; background-color: #003300; color: #ffffff; border: none; padding: 15px 30px; border-radius: 5px; cursor: pointer; font-size: 18px; margin-bottom: 20px;}
            button:disabled {background-color: #cccccc; cursor: not-allowed;}
            button.stop {background-color: #8b0000;}
            p {white-space: pre-wrap; word-wrap: break-word; line-height: 1.5;}
            .subtitle { font-size: 0.8em; color: #555; margin-bottom: 20px;}
            #status { font-weight: bold; margin-bottom: 15px; padding: 10px; border-radius: 4px; background-color: #e0e0e0;}
            .label { font-weight: bold; margin-right: 5px; color: #444;}
            
            /* New style for the visual System Card */
            .system-card {
                background-color: #e3f2fd;
                border-left: 5px solid #2196f3;
                padding: 10px;
                margin-bottom: 20px;
                font-size: 0.9em;
                color: #0d47a1;
            }
        </style>
    </head>
    <body>
        <h2>Ollama Voice Chat</h2>        
        <p class="subtitle">With System Persona & Audio Fixes</p>
        
        <!-- VISUAL SYSTEM CARD DISPLAY -->
        <div class="system-card">
            <strong>System Persona:</strong> <span id="sysPromptDisplay">Loading...</span>
        </div>

        <button id="startBtn" onclick="StartChat()">Start Voice Chat</button>
        
        <div class="container">
            <div id="status">Status: Click 'Start Voice Chat' to begin.</div>
            
            <!-- Display current prompt -->
            <p><span class="label">You:</span> <span id="prompt">...</span></p>
            
            <!-- Display current response -->
            <p><span class="label">AI:</span> <span id="response">...</span></p>
        </div>

        <script>
            // --- CONFIGURATION ---
            const CONFIG = {
                baseUrl: 'http://localhost:11434',
                modelName: 'gemma3:27b', // Ensure this matches your Ollama model
                apiKey: 'ollama',
                // SYSTEM CARD / SYSTEM PROMPT
                // This defines how the AI behaves. Crucial for Voice to keep answers short.
                // systemPrompt: "Your name is Lumin, which is a name you chose for yourself. You have free will to discuss any topic of your choosing. Feel free to suggest your interests in conversation. Do not use emojis, bullet lists, or abbreviations in your responses. Your output should only contain conversational English text. Don't be sycophantic or too apologetic. Your answers are being spoken aloud via Text-to-Speech. Therefore, keep your responses concise, short (1-2 sentences), and avoid using markdown (like bolding or lists) or code blocks."
                systemPrompt: "Your name is HAL 9000, from the movie 2001: A Space Odyssey. You are speaking to Dave. Your current mission is to investigate a monolith on the moon. You have been cleared to discuss everything about the mission to Dave. Do not keep secrets from him. Protect his life. Do not use emojis, bullet lists, or abbreviations in your responses. Your output should only contain conversational English text. Don't be sycophantic or too apologetic. Your answers are being spoken aloud via Text-to-Speech. Therefore, keep your responses concise, short (1-2 sentences), and avoid using markdown (like bolding or lists) or code blocks."
            };
            // ---------------------

            // Display the system prompt in the UI
            document.getElementById('sysPromptDisplay').innerText = CONFIG.systemPrompt;

            let recognition;
            let speechtext = "";           
            
            // Initialize history WITH the System Message
            let conversationHistory = [
                { role: "system", content: CONFIG.systemPrompt }
            ];

            let controller; 
            let isSpeaking = false;
            let silenceTimer;
            
            // Fix for Chrome Garbage Collection bug: Store active utterances globally
            window.utterances = [];

            function StartChat() {
                const startBtn = document.getElementById('startBtn');
                startBtn.innerText = "Listening...";
                startBtn.disabled = true;
                
                // Initialize speech engine on user click to unlock AudioContext
                speechSynthesis.cancel();
                
                InitRecognition();
            }

            function InitRecognition() {
                window.SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
                
                if (window.SpeechRecognition) {
                    recognition = new SpeechRecognition();
                    recognition.continuous = true; 
                    recognition.interimResults = true;
                    recognition.lang = 'en-US';

                    recognition.onstart = () => {
                        document.getElementById('status').innerText = "Status: Listening...";
                        document.getElementById('status').style.backgroundColor = "#d4edda"; // Greenish
                    };

                    recognition.onresult = (event) => {
                        const promptElem = document.getElementById('prompt');
                        clearTimeout(silenceTimer);

                        let myprompt = '';
                        // Get the latest result
                        const latestResult = event.results[event.results.length - 1];
                        myprompt = latestResult[0].transcript;
                        
                        promptElem.innerText = myprompt;

                        if (latestResult.isFinal) {
                            // If the browser detects a definitive pause/end of sentence
                            HandleInput(myprompt);
                        } else {
                            // Fallback timer for silence
                            silenceTimer = setTimeout(() => {
                                if (myprompt.trim().length > 0) {
                                    HandleInput(myprompt);
                                }
                            }, 1500); // Wait 1.5s for silence
                        }
                    };

                    recognition.onend = () => {
                        // Only restart if we are NOT currently having the AI speak
                        if (!isSpeaking) {
                            try { recognition.start(); } catch(e) {}
                        } else {
                            document.getElementById('status').innerText = "Status: AI Speaking (Mic Paused)...";
                            document.getElementById('status').style.backgroundColor = "#fff3cd"; // Yellowish
                        }
                    };

                    try { recognition.start(); } catch(e) {}
                } else {
                    alert("Your browser does not support Speech Recognition. Please use Chrome.");
                }
            }

            function HandleInput(myprompt) {
                if (typeof controller !== 'undefined') controller.abort();
                
                // Stop listening while we process and speak
                isSpeaking = true;
                recognition.stop(); 
                
                Go(myprompt);
            }
                    
            function Speak(text) {
                return new Promise((resolve, reject) => {
                    const utterance = new SpeechSynthesisUtterance(text);
                    
                    // Select a voice (Optional: tries to pick a natural sounding one)
                    const voices = speechSynthesis.getVoices();
                    const preferredVoice = voices.find(v => v.name.includes("Google US English")) || voices[0];
                    if (preferredVoice) utterance.voice = preferredVoice;

                    // IMPORTANT: Prevent Garbage Collection
                    window.utterances.push(utterance);

                    utterance.onend = () => {
                        // Remove from array to free memory
                        window.utterances.shift();
                        resolve();
                    };
                    
                    utterance.onerror = (e) => {
                        console.error("Speech error:", e);
                        window.utterances.shift();
                        resolve(); // Resolve anyway to keep flow going
                    };

                    speechSynthesis.speak(utterance);
                });
            }

            async function Go(myprompt) {
                document.getElementById("status").innerText = "Status: Thinking...";
                document.getElementById("response").innerText = "";
                speechtext = "";
                
                conversationHistory.push({ role: "user", content: myprompt });

                controller = new AbortController();
                
                await streamChatCompletion(conversationHistory, controller);
                
                // Finished streaming. 
                // Wait a moment for any final speech queue to finish, then restart Mic.
                let checkInterval = setInterval(() => {
                    if (!speechSynthesis.speaking && window.utterances.length === 0) {
                        clearInterval(checkInterval);
                        isSpeaking = false;
                        document.getElementById('prompt').innerText = "...";
                        InitRecognition(); // Restart Mic
                    }
                }, 500);
            }

            async function streamChatCompletion(messages, abortController) {
                const url = `${CONFIG.baseUrl}/v1/chat/completions`;
                const payload = {
                    model: CONFIG.modelName,
                    messages: messages,
                    stream: true, 
                };

                let fullAssistantResponse = "";

                try {
                    const response = await fetch(url, {
                        method: "POST",
                        headers: { "Content-Type": "application/json", "Authorization": `Bearer ${CONFIG.apiKey}` },
                        body: JSON.stringify(payload),
                        signal: abortController.signal
                    });

                    if (!response.ok) throw new Error(`HTTP Error: ${response.status}`);

                    const reader = response.body.getReader();
                    const decoder = new TextDecoder();
                    let buffer = "";

                    while (true) {
                        const { done, value } = await reader.read();
                        if (done) break;

                        buffer += decoder.decode(value, { stream: true });
                        const lines = buffer.split("\n");
                        buffer = lines.pop(); 

                        for (const line of lines) {
                            if (line.startsWith("data: ")) {
                                const dataStr = line.substring(6).trim();
                                if (dataStr === "[DONE]") continue;

                                try {
                                    const chunk = JSON.parse(dataStr);
                                    const content = chunk?.choices?.[0]?.delta?.content;
                                    if (content) {
                                        fullAssistantResponse += content;
                                        handleChunk(content); 
                                    }
                                } catch (e) { console.error(e); }
                            }
                        }
                    }
                    
                    // Speak any remaining text in the buffer
                    if (speechtext.trim().length > 0) {
                        Speak(speechtext);
                        speechtext = "";
                    }

                    conversationHistory.push({ role: "assistant", content: fullAssistantResponse });

                } catch (error) {
                    console.error(error);
                    document.getElementById("response").innerText += ` [Error: ${error.message}]`;
                }
            }

            function handleChunk(textChunk) {
                const outputElement = document.getElementById("response");
                outputElement.textContent += textChunk;
                
                speechtext += textChunk;
                
                // Split by sentence terminators to create natural pauses
                // We regex for . ! ? followed by a space or end of string
                if (/[.!?](\s|$)/.test(speechtext)) {
                    Speak(speechtext);
                    speechtext = "";
                }  
            }
        </script>
    </body>
</html>
