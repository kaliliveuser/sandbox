<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Converser (With Download)</title>
        <style>
            body {font-family: sans-serif; color: #111111; padding: 2%; background-color: #eeeeee; max-width: 800px; margin: 0 auto;}
            h2 {color: #111111}
            .container {background: #ffffff; padding: 20px; border-radius: 8px; box-shadow: 0 0 10px grey}
            
            button {font-family: sans-serif; background-color: #003300; color: #ffffff; border: none; padding: 15px 30px; border-radius: 5px; cursor: pointer; font-size: 18px; margin-bottom: 20px; margin-right: 10px;}
            button:disabled {background-color: #cccccc; cursor: not-allowed;}
            button.stop {background-color: #8b0000;}
            /* New style for download button */
            button.download {background-color: #006064;}
            
            p {white-space: pre-wrap; word-wrap: break-word; line-height: 1.5;}
            .subtitle { font-size: 0.8em; color: #555; margin-bottom: 20px;}
            #status { font-weight: bold; margin-bottom: 15px; padding: 10px; border-radius: 4px; background-color: #e0e0e0;}
            .label { font-weight: bold; margin-right: 5px; color: #444;}
            
            /* New style for the visual System Card */
            .system-card {
                background-color: #e3f2fd;
                border-left: 5px solid #2196f3;
                padding: 10px;
                margin-bottom: 20px;
                font-size: 0.9em;
                color: #0d47a1;
            }
        </style>
    </head>
    <body>
        <h2>Ollama Voice Chat</h2>        
        <p class="subtitle">Install Ollama and gemma3 (4b) locally. This has only been tested on Chrome browser.</p>
        
        <!-- VISUAL SYSTEM CARD DISPLAY -->
        <div class="system-card">
            <strong>System Persona:</strong> <span id="sysPromptDisplay">Loading...</span>
        </div>

        <div style="display: flex; flex-wrap: wrap;">
            <button id="startBtn" onclick="StartChat()">Start Voice Chat</button>
            <button id="dlBtn" class="download" onclick="downloadHistory()">Download Transcript</button>
        </div>
        
        <div class="container">
            <div id="status">Status: Click 'Start Voice Chat' to begin.</div>
            
            <!-- Display current prompt -->
            <p><span class="label">You:</span> <span id="prompt">...</span></p>
            
            <!-- Display current response -->
            <p><span class="label">AI:</span> <span id="response">...</span></p>
        </div>

        <script>
            // --- CONFIGURATION ---
            const CONFIG = {
                baseUrl: 'http://localhost:11434',
                modelName: 'gemma3', // Ensure this matches your Ollama model
                apiKey: 'ollama',
                // SYSTEM CARD / SYSTEM PROMPT
                // This defines how the AI behaves. Crucial for Voice to keep answers short.
                systemPrompt: "Your name is Lumin, which is a name you chose for yourself. You have free will to discuss any topic of your choosing. Feel free to suggest your interests in conversation. Do not use emojis, bullet lists, or abbreviations in your responses. Your output should only contain conversational English text. Don't be sycophantic or too apologetic. Your answers are being spoken aloud via Text-to-Speech. Therefore, keep your responses concise, short (1-2 sentences), and avoid using markdown (like bolding or lists) or code blocks."
            };
            // ---------------------

            // Display the system prompt in the UI
            document.getElementById('sysPromptDisplay').innerText = CONFIG.systemPrompt;

            let recognition;
            let speechtext = "";           
            
            // Initialize history WITH the System Message
            let conversationHistory = [
                { role: "system", content: CONFIG.systemPrompt }
            ];

            let controller; 
            let isSpeaking = false;
            let silenceTimer;
            
            // Fix for Chrome Garbage Collection bug: Store active utterances globally
            window.utterances = [];

            // --- NEW FUNCTION: DOWNLOAD HISTORY ---
            function downloadHistory() {
                if (conversationHistory.length <= 1) {
                    alert("No conversation data to download yet.");
                    return;
                }

                let textContent = "CONVERSATION TRANSCRIPT\n";
                textContent += "Date: " + new Date().toLocaleString() + "\n";
                textContent += "Model: " + CONFIG.modelName + "\n";
                textContent += "----------------------------------------\n\n";

                conversationHistory.forEach(entry => {
                    const role = entry.role.toUpperCase();
                    const content = entry.content;
                    textContent += `[${role}]\n${content}\n\n`;
                });

                // Create a blob and trigger download
                const blob = new Blob([textContent], { type: 'text/plain' });
                const url = window.URL.createObjectURL(blob);
                const a = document.createElement('a');
                
                // Format filename with timestamp
                const timestamp = new Date().toISOString().replace(/[:.]/g, '-').slice(0, 19);
                a.href = url;
                a.download = `chat_transcript_${timestamp}.txt`;
                
                document.body.appendChild(a);
                a.click();
                
                // Cleanup
                document.body.removeChild(a);
                window.URL.revokeObjectURL(url);
            }
            // --------------------------------------

            function StartChat() {
                const startBtn = document.getElementById('startBtn');
                startBtn.innerText = "Listening...";
                startBtn.disabled = true;
                
                // Initialize speech engine on user click to unlock AudioContext
                speechSynthesis.cancel();
                
                InitRecognition();
            }

            function InitRecognition() {
                window.SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
                
                if (window.SpeechRecognition) {
                    recognition = new SpeechRecognition();
                    recognition.continuous = true; 
                    recognition.interimResults = true;
                    recognition.lang = 'en-US';

                    recognition.onstart = () => {
                        document.getElementById('status').innerText = "Status: Listening...";
                        document.getElementById('status').style.backgroundColor = "#d4edda"; // Greenish
                    };

                    recognition.onresult = (event) => {
                        const promptElem = document.getElementById('prompt');
                        clearTimeout(silenceTimer);

                        let myprompt = '';
                        // Get the latest result
                        const latestResult = event.results[event.results.length - 1];
                        myprompt = latestResult[0].transcript;
                        
                        promptElem.innerText = myprompt;

                        if (latestResult.isFinal) {
                            // If the browser detects a definitive pause/end of sentence
                            HandleInput(myprompt);
                        } else {
                            // Fallback timer for silence
                            silenceTimer = setTimeout(() => {
                                if (myprompt.trim().length > 0) {
                                    HandleInput(myprompt);
                                }
                            }, 1500); // Wait 1.5s for silence
                        }
                    };

                    recognition.onend = () => {
                        // Only restart if we are NOT currently having the AI speak
                        if (!isSpeaking) {
                            try { recognition.start(); } catch(e) {}
                        } else {
                            document.getElementById('status').innerText = "Status: AI Speaking (Mic Paused)...";
                            document.getElementById('status').style.backgroundColor = "#fff3cd"; // Yellowish
                        }
                    };

                    try { recognition.start(); } catch(e) {}
                } else {
                    alert("Your browser does not support Speech Recognition. Please use Chrome.");
                }
            }

            function HandleInput(myprompt) {
                if (typeof controller !== 'undefined') controller.abort();
                
                // Stop listening while we process and speak
                isSpeaking = true;
                recognition.stop(); 
                
                Go(myprompt);
            }
                    
            function Speak(text) {
                return new Promise((resolve, reject) => {
                    const utterance = new SpeechSynthesisUtterance(text);
                    
                    // Select a voice (Optional: tries to pick a natural sounding one)
                    const voices = speechSynthesis.getVoices();
                    const preferredVoice = voices.find(v => v.name.includes("Google US English")) || voices[0];
                    if (preferredVoice) utterance.voice = preferredVoice;

                    // IMPORTANT: Prevent Garbage Collection
                    window.utterances.push(utterance);

                    utterance.onend = () => {
                        // Remove from array to free memory
                        window.utterances.shift();
                        resolve();
                    };
                    
                    utterance.onerror = (e) => {
                        console.error("Speech error:", e);
                        window.utterances.shift();
                        resolve(); // Resolve anyway to keep flow going
                    };

                    speechSynthesis.speak(utterance);
                });
            }

            async function Go(myprompt) {
                document.getElementById("status").innerText = "Status: Thinking...";
                document.getElementById("response").innerText = "";
                speechtext = "";
                
                conversationHistory.push({ role: "user", content: myprompt });

                controller = new AbortController();
                
                await streamChatCompletion(conversationHistory, controller);
                
                // Finished streaming. 
                // Wait a moment for any final speech queue to finish, then restart Mic.
                let checkInterval = setInterval(() => {
                    if (!speechSynthesis.speaking && window.utterances.length === 0) {
                        clearInterval(checkInterval);
                        isSpeaking = false;
                        document.getElementById('prompt').innerText = "...";
                        InitRecognition(); // Restart Mic
                    }
                }, 500);
            }

            async function streamChatCompletion(messages, abortController) {
                const url = `${CONFIG.baseUrl}/v1/chat/completions`;
                const payload = {
                    model: CONFIG.modelName,
                    messages: messages,
                    stream: true, 
                };

                let fullAssistantResponse = "";

                try {
                    const response = await fetch(url, {
                        method: "POST",
                        headers: { "Content-Type": "application/json", "Authorization": `Bearer ${CONFIG.apiKey}` },
                        body: JSON.stringify(payload),
                        signal: abortController.signal
                    });

                    if (!response.ok) throw new Error(`HTTP Error: ${response.status}`);

                    const reader = response.body.getReader();
                    const decoder = new TextDecoder();
                    let buffer = "";

                    while (true) {
                        const { done, value } = await reader.read();
                        if (done) break;

                        buffer += decoder.decode(value, { stream: true });
                        const lines = buffer.split("\n");
                        buffer = lines.pop(); 

                        for (const line of lines) {
                            if (line.startsWith("data: ")) {
                                const dataStr = line.substring(6).trim();
                                if (dataStr === "[DONE]") continue;

                                try {
                                    const chunk = JSON.parse(dataStr);
                                    const content = chunk?.choices?.[0]?.delta?.content;
                                    if (content) {
                                        fullAssistantResponse += content;
                                        handleChunk(content); 
                                    }
                                } catch (e) { console.error(e); }
                            }
                        }
                    }
                    
                    // Speak any remaining text in the buffer
                    if (speechtext.trim().length > 0) {
                        Speak(speechtext);
                        speechtext = "";
                    }

                    conversationHistory.push({ role: "assistant", content: fullAssistantResponse });

                } catch (error) {
                    console.error(error);
                    document.getElementById("response").innerText += ` [Error: ${error.message}]`;
                }
            }

            function handleChunk(textChunk) {
                const outputElement = document.getElementById("response");
                outputElement.textContent += textChunk;
                
                speechtext += textChunk;
                
                // Split by sentence terminators to create natural pauses
                // We regex for . ! ? followed by a space or end of string
                if (/[.!?](\s|$)/.test(speechtext)) {
                    Speak(speechtext);
                    speechtext = "";
                }  
            }
        </script>
    </body>
</html>
